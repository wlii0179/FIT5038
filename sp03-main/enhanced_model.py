#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Machine Learning Model for Phishing Detection
ä½¿ç”¨å¤šç§å…ˆè¿›ç®—æ³•çš„é›†æˆå­¦ä¹ ç³»ç»Ÿ
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import (
    RandomForestClassifier, 
    GradientBoostingClassifier, 
    AdaBoostClassifier,
    VotingClassifier,
    BaggingClassifier
)
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    classification_report, 
    confusion_matrix, 
    roc_auc_score,
    precision_recall_curve,
    f1_score
)
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import xgboost as xgb
import warnings
import pickle
import time
warnings.filterwarnings('ignore')

class EnhancedPhishingDetector:
    """å¢å¼ºçš„é’“é±¼æ£€æµ‹æ¨¡å‹é›†æˆç³»ç»Ÿ"""
    
    def __init__(self):
        self.models = {}
        self.ensemble_model = None
        self.scaler = StandardScaler()
        self.feature_names = None
        self.training_scores = {}
        
    def create_base_models(self):
        """åˆ›å»ºåŸºç¡€æ¨¡å‹é›†åˆ"""
        print("ğŸ”§ åˆ›å»ºåŸºç¡€æ¨¡å‹...")
        
        self.models = {
            # æ ‘æ¨¡å‹
            'rf': RandomForestClassifier(
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            ),
            
            'gb': GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                min_samples_split=5,
                random_state=42
            ),
            
            'xgb': xgb.XGBClassifier(
                n_estimators=200,
                learning_rate=0.1,
                max_depth=8,
                min_child_weight=3,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                eval_metric='logloss'
            ),
            
            'ada': AdaBoostClassifier(
                n_estimators=100,
                learning_rate=1.0,
                random_state=42
            ),
            
            # çº¿æ€§æ¨¡å‹
            'lr': LogisticRegression(
                C=1.0,
                max_iter=1000,
                random_state=42
            ),
            
            # æ”¯æŒå‘é‡æœº
            'svm': SVC(
                C=1.0,
                kernel='rbf',
                gamma='scale',
                probability=True,
                random_state=42
            ),
            
            # ç¥ç»ç½‘ç»œ
            'mlp': MLPClassifier(
                hidden_layer_sizes=(100, 50),
                activation='relu',
                solver='adam',
                alpha=0.001,
                learning_rate='adaptive',
                max_iter=500,
                random_state=42
            ),
            
            # æœ´ç´ è´å¶æ–¯
            'nb': GaussianNB(),
            
            # Kè¿‘é‚»
            'knn': KNeighborsClassifier(
                n_neighbors=7,
                weights='distance'
            )
        }
        
        print(f"âœ… åˆ›å»ºäº† {len(self.models)} ä¸ªåŸºç¡€æ¨¡å‹")
        
    def train_individual_models(self, X_train, y_train, X_test, y_test):
        """è®­ç»ƒå„ä¸ªåŸºç¡€æ¨¡å‹"""
        print("\nğŸš€ è®­ç»ƒå„ä¸ªåŸºç¡€æ¨¡å‹...")
        
        # å°†æ ‡ç­¾ä» {-1, 1} æ˜ å°„åˆ° {0, 1} ä»¥å…¼å®¹XGBoost
        y_train_mapped = np.where(y_train == -1, 0, 1)
        y_test_mapped = np.where(y_test == -1, 0, 1)
        
        # åˆ›å»ºæ¨¡å‹åˆ—è¡¨çš„å‰¯æœ¬ä»¥é¿å…è¿­ä»£æ—¶ä¿®æ”¹å­—å…¸
        models_list = list(self.models.items())
        
        for name, model in models_list:
            print(f"\nè®­ç»ƒ {name.upper()} æ¨¡å‹...")
            start_time = time.time()
            
            try:
                # å¯¹XGBoostä½¿ç”¨æ˜ å°„åçš„æ ‡ç­¾
                current_y_train = y_train_mapped if name == 'xgb' else y_train
                current_y_test = y_test_mapped if name == 'xgb' else y_test
                
                # å¯¹éœ€è¦æ ‡å‡†åŒ–çš„æ¨¡å‹è¿›è¡Œé¢„å¤„ç†
                if name in ['svm', 'mlp', 'knn', 'lr']:
                    # åˆ›å»ºåŒ…å«æ ‡å‡†åŒ–çš„ç®¡é“
                    pipeline = Pipeline([
                        ('scaler', StandardScaler()),
                        ('classifier', model)
                    ])
                    pipeline.fit(X_train, current_y_train)
                    self.models[name] = pipeline
                    
                    # è¯„ä¼°
                    train_score = pipeline.score(X_train, current_y_train)
                    test_score = pipeline.score(X_test, current_y_test)
                    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]
                else:
                    # æ ‘æ¨¡å‹å’Œæœ´ç´ è´å¶æ–¯ä¸éœ€è¦æ ‡å‡†åŒ–
                    model.fit(X_train, current_y_train)
                    train_score = model.score(X_train, current_y_train)
                    test_score = model.score(X_test, current_y_test)
                    y_pred_proba = model.predict_proba(X_test)[:, 1]
                
                # è®¡ç®—AUC (ä½¿ç”¨åŸå§‹æ ‡ç­¾)
                auc_score = roc_auc_score(y_test, y_pred_proba)
                training_time = time.time() - start_time
                
                self.training_scores[name] = {
                    'train_acc': train_score,
                    'test_acc': test_score,
                    'auc': auc_score,
                    'time': training_time
                }
                
                print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_score:.4f}")
                print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_score:.4f}")
                print(f"  AUCåˆ†æ•°: {auc_score:.4f}")
                print(f"  è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
                
            except Exception as e:
                print(f"  âŒ {name} æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
                if name in self.models:
                    del self.models[name]
    
    def create_ensemble_model(self, X_train, y_train):
        """åˆ›å»ºé›†æˆæ¨¡å‹"""
        print("\nğŸ¯ åˆ›å»ºé›†æˆæ¨¡å‹...")
        
        # é€‰æ‹©è¡¨ç°æœ€å¥½çš„æ¨¡å‹
        valid_models = []
        for name, model in self.models.items():
            if name in self.training_scores:
                score = self.training_scores[name]['test_acc']
                if score > 0.90:  # åªé€‰æ‹©æµ‹è¯•å‡†ç¡®ç‡>90%çš„æ¨¡å‹
                    valid_models.append((name, model))
        
        print(f"é€‰æ‹© {len(valid_models)} ä¸ªä¼˜ç§€æ¨¡å‹è¿›è¡Œé›†æˆ:")
        for name, _ in valid_models:
            score = self.training_scores[name]
            print(f"  - {name.upper()}: {score['test_acc']:.4f} (AUC: {score['auc']:.4f})")
        
        # åˆ›å»ºæŠ•ç¥¨åˆ†ç±»å™¨
        if len(valid_models) >= 3:
            self.ensemble_model = VotingClassifier(
                estimators=valid_models,
                voting='soft'  # ä½¿ç”¨æ¦‚ç‡æŠ•ç¥¨
            )
            
            print("\nè®­ç»ƒé›†æˆæ¨¡å‹...")
            # æ£€æŸ¥æ˜¯å¦åŒ…å«XGBoostï¼Œå¦‚æœæ˜¯åˆ™éœ€è¦æ˜ å°„æ ‡ç­¾
            has_xgb = any(name == 'xgb' for name, _ in valid_models)
            if has_xgb:
                # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…è£…å™¨æ¥å¤„ç†XGBoostçš„æ ‡ç­¾æ˜ å°„
                print("âš ï¸ æ£€æµ‹åˆ°XGBoostï¼Œä½¿ç”¨åŸå§‹æ ‡ç­¾è®­ç»ƒé›†æˆæ¨¡å‹...")
            
            self.ensemble_model.fit(X_train, y_train)
            print("âœ… é›†æˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
        else:
            print("âŒ ä¼˜ç§€æ¨¡å‹æ•°é‡ä¸è¶³ï¼Œä½¿ç”¨æœ€ä½³å•ä¸€æ¨¡å‹")
            best_model_name = max(self.training_scores.keys(), 
                                key=lambda x: self.training_scores[x]['test_acc'])
            self.ensemble_model = self.models[best_model_name]
            print(f"âœ… é€‰æ‹©æœ€ä½³æ¨¡å‹: {best_model_name.upper()}")
    
    def evaluate_ensemble(self, X_test, y_test):
        """è¯„ä¼°é›†æˆæ¨¡å‹"""
        print("\nğŸ“Š è¯„ä¼°é›†æˆæ¨¡å‹æ€§èƒ½...")
        
        # é¢„æµ‹
        y_pred = self.ensemble_model.predict(X_test)
        y_pred_proba = self.ensemble_model.predict_proba(X_test)[:, 1]
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        accuracy = self.ensemble_model.score(X_test, y_test)
        auc = roc_auc_score(y_test, y_pred_proba)
        f1 = f1_score(y_test, y_pred)
        
        print(f"é›†æˆæ¨¡å‹æ€§èƒ½:")
        print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
        print(f"  AUCåˆ†æ•°: {auc:.4f}")
        print(f"  F1åˆ†æ•°: {f1:.4f}")
        
        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, target_names=['Phishing(-1)', 'Safe(1)']))
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_test, y_pred)
        print("\næ··æ·†çŸ©é˜µ:")
        print(f"çœŸè´Ÿä¾‹(TN): {cm[0,0]}, å‡æ­£ä¾‹(FP): {cm[0,1]}")
        print(f"å‡è´Ÿä¾‹(FN): {cm[1,0]}, çœŸæ­£ä¾‹(TP): {cm[1,1]}")
        
        return {
            'accuracy': accuracy,
            'auc': auc,
            'f1': f1,
            'confusion_matrix': cm
        }
    
    def save_enhanced_model(self, filepath='pickle/enhanced_model.pkl'):
        """ä¿å­˜å¢å¼ºæ¨¡å‹"""
        model_data = {
            'ensemble_model': self.ensemble_model,
            'training_scores': self.training_scores,
            'feature_names': self.feature_names
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"âœ… å¢å¼ºæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def predict_enhanced(self, X):
        """ä½¿ç”¨å¢å¼ºæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        if self.ensemble_model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train() æ–¹æ³•")
        
        # è·å–é¢„æµ‹æ¦‚ç‡
        proba = self.ensemble_model.predict_proba(X)
        return proba[:, 1]  # è¿”å›æ­£ç±»(å®‰å…¨)æ¦‚ç‡
    
    def train(self, csv_file='phishing.csv'):
        """å®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        print("ğŸ¯ å¼€å§‹å¢å¼ºæ¨¡å‹è®­ç»ƒæµç¨‹")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        print("ğŸ“‚ åŠ è½½æ•°æ®...")
        df = pd.read_csv(csv_file)
        X = df.drop(['class', 'Index'], axis=1)
        y = df['class']
        self.feature_names = X.columns.tolist()
        
        print(f"æ•°æ®è§„æ¨¡: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {dict(y.value_counts())}")
        
        # 2. æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        print(f"è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")
        
        # 3. åˆ›å»ºå’Œè®­ç»ƒåŸºç¡€æ¨¡å‹
        self.create_base_models()
        self.train_individual_models(X_train, y_train, X_test, y_test)
        
        # 4. åˆ›å»ºé›†æˆæ¨¡å‹
        self.create_ensemble_model(X_train, y_train)
        
        # 5. è¯„ä¼°é›†æˆæ¨¡å‹
        results = self.evaluate_ensemble(X_test, y_test)
        
        # 6. ä¿å­˜æ¨¡å‹
        self.save_enhanced_model()
        
        print("\nğŸ‰ å¢å¼ºæ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨å¢å¼ºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
    
    # åˆ›å»ºå¢å¼ºæ£€æµ‹å™¨
    detector = EnhancedPhishingDetector()
    
    # è®­ç»ƒæ¨¡å‹
    results = detector.train()
    
    print("\nğŸ“ˆ æœ€ç»ˆç»“æœæ€»ç»“:")
    print(f"å¢å¼ºæ¨¡å‹å‡†ç¡®ç‡: {results['accuracy']:.4f}")
    print(f"AUCåˆ†æ•°: {results['auc']:.4f}")
    print(f"F1åˆ†æ•°: {results['f1']:.4f}")

if __name__ == "__main__":
    main() 